{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anudwigna/projects/llm_experiments/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, WikipediaReader\n",
    "from IPython.display import Markdown, display\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hhh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs = WikipediaReader().load_data(pages=['Toronto', 'Berlin', 'Tokyo'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create db schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_hf=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.2, \"max_length\":512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SQLDatabase, ServiceContext, GPTSQLStructStoreIndex\n",
    "from llama_index import LLMPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`transformers` package not found, please run `pip install transformers`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.8/site-packages/llama_index/utils.py:54\u001b[0m, in \u001b[0;36mGlobalsHelper.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m llm_predictor \u001b[39m=\u001b[39m LLMPredictor(llm\u001b[39m=\u001b[39mllm_hf)\n\u001b[0;32m----> 2\u001b[0m service_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults(llm_predictor\u001b[39m=\u001b[39;49mllm_predictor)\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.8/site-packages/llama_index/indices/service_context.py:146\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    143\u001b[0m llm_predictor\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[1;32m    145\u001b[0m \u001b[39m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m embed_model \u001b[39m=\u001b[39m embed_model \u001b[39mor\u001b[39;00m OpenAIEmbedding()\n\u001b[1;32m    147\u001b[0m embed_model\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[1;32m    149\u001b[0m prompt_helper \u001b[39m=\u001b[39m prompt_helper \u001b[39mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[1;32m    150\u001b[0m     llm_metadata\u001b[39m=\u001b[39mllm_predictor\u001b[39m.\u001b[39mget_llm_metadata(),\n\u001b[1;32m    151\u001b[0m     context_window\u001b[39m=\u001b[39mcontext_window,\n\u001b[1;32m    152\u001b[0m     num_output\u001b[39m=\u001b[39mnum_output,\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.8/site-packages/llama_index/embeddings/openai.py:227\u001b[0m, in \u001b[0;36mOpenAIEmbedding.__init__\u001b[0;34m(self, mode, model, deployment_name, embed_batch_size, tokenizer, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    218\u001b[0m     mode: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m OpenAIEmbeddingMode\u001b[39m.\u001b[39mTEXT_SEARCH_MODE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    225\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Init params.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(embed_batch_size, tokenizer, callback_manager)\n\u001b[1;32m    228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeployment_name \u001b[39m=\u001b[39m deployment_name\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_engine \u001b[39m=\u001b[39m get_engine(mode, model, _QUERY_MODE_MODEL_DICT)\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.8/site-packages/llama_index/embeddings/base.py:62\u001b[0m, in \u001b[0;36mBaseEmbedding.__init__\u001b[0;34m(self, embed_batch_size, tokenizer, callback_manager)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total_tokens_used \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_token_usage: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m tokenizer \u001b[39mor\u001b[39;00m globals_helper\u001b[39m.\u001b[39;49mtokenizer\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager \u001b[39mor\u001b[39;00m CallbackManager([])\n\u001b[1;32m     64\u001b[0m \u001b[39m# list of tuples of id, text\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.8/site-packages/llama_index/utils.py:56\u001b[0m, in \u001b[0;36mGlobalsHelper.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`transformers` package not found, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mplease run `pip install transformers`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     61\u001b[0m tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mGPT2TokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenizer_fn\u001b[39m(text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n",
      "\u001b[0;31mImportError\u001b[0m: `transformers` package not found, please run `pip install transformers`"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=llm_hf)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCREATE TABLE city_stats (\\n\\tcity_name VARCHAR(16) NOT NULL, \\n\\tpopulation INTEGER, \\n\\tcountry VARCHAR(16) NOT NULL, \\n\\tPRIMARY KEY (city_name)\\n)\\n\\n/*\\n3 rows from city_stats table:\\ncity_name\\tpopulation\\tcountry\\n\\n*/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_database.table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# NOTE: the table_name specified here is the table that you\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# want to extract into from unstructured documents.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# index = GPTSQLStructStoreIndex.from_documents(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m#     service_context=service_context\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m index \u001b[39m=\u001b[39m GPTSQLStructStoreIndex(\n\u001b[1;32m     11\u001b[0m     [],\n\u001b[1;32m     12\u001b[0m     sql_database\u001b[39m=\u001b[39;49msql_database, \n\u001b[1;32m     13\u001b[0m     table_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcity_stats\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.10/site-packages/llama_index/indices/struct_store/sql.py:83\u001b[0m, in \u001b[0;36mGPTSQLStructStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, sql_database, table_name, table, ref_doc_id_column, sql_context_container, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m index_struct \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     nodes \u001b[39m=\u001b[39m nodes \u001b[39mor\u001b[39;00m []\n\u001b[0;32m---> 83\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     84\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     85\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     86\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     87\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[39m# TODO: index_struct context_dict is deprecated,\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m# we're migrating storage of information to here.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m sql_context_container \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.10/site-packages/llama_index/indices/struct_store/base.py:55\u001b[0m, in \u001b[0;36mBaseGPTStructStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, schema_extract_prompt, output_parser, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema_extract_prompt \u001b[39m=\u001b[39m (\n\u001b[1;32m     52\u001b[0m     schema_extract_prompt \u001b[39mor\u001b[39;00m DEFAULT_SCHEMA_EXTRACT_PROMPT\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_parser \u001b[39m=\u001b[39m output_parser \u001b[39mor\u001b[39;00m default_output_parser\n\u001b[0;32m---> 55\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     56\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     57\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     58\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     59\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     60\u001b[0m )\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.10/site-packages/llama_index/indices/base.py:58\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnodes must be a list of Node objects.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context \u001b[39m=\u001b[39m service_context \u001b[39mor\u001b[39;00m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults()\n\u001b[1;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context \u001b[39m=\u001b[39m storage_context \u001b[39mor\u001b[39;00m StorageContext\u001b[39m.\u001b[39mfrom_defaults()\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mdocstore\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.10/site-packages/llama_index/indices/service_context.py:84\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, chunk_size_limit)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mIf an argument is specified, then use the argument value provided for that\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mparameter. If an argument is not specified, then use the default value.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m callback_manager \u001b[39m=\u001b[39m callback_manager \u001b[39mor\u001b[39;00m CallbackManager([])\n\u001b[0;32m---> 84\u001b[0m llm_predictor \u001b[39m=\u001b[39m llm_predictor \u001b[39mor\u001b[39;00m LLMPredictor()\n\u001b[1;32m     85\u001b[0m llm_predictor\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[1;32m     87\u001b[0m \u001b[39m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:177\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[0;34m(self, llm, retry_on_throttling, cache, callback_manager)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    171\u001b[0m     llm: Optional[BaseLanguageModel] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m     callback_manager: Optional[CallbackManager] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    175\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m llm \u001b[39mor\u001b[39;00m OpenAI(temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m         langchain\u001b[39m.\u001b[39mllm_cache \u001b[39m=\u001b[39m cache\n",
      "File \u001b[0;32m~/projects/llm_experiments/venv/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "# NOTE: the table_name specified here is the table that you\n",
    "# want to extract into from unstructured documents.\n",
    "# index = GPTSQLStructStoreIndex.from_documents(\n",
    "#     wiki_docs, \n",
    "#     sql_database=sql_database, \n",
    "#     table_name=\"city_stats\",\n",
    "#     service_context=service_context\n",
    "# )\n",
    "\n",
    "index = GPTSQLStructStoreIndex(\n",
    "    [],\n",
    "    sql_database=sql_database, \n",
    "    table_name=\"city_stats\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view current table\n",
    "stmt = select(\n",
    "    city_stats_table.c[\"city_name\", \"population\", \"country\"]\n",
    ").select_from(city_stats_table)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(stmt).fetchall()\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    query_mode=\"sql\"\n",
    ")\n",
    "response = query_engine.query(\"SELECT city_name from city_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine(\n",
    "    query_mode=\"nl\"\n",
    ")\n",
    "response = query_engine.query(\"Which city has the lowest population?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also fetch the raw result from SQLAlchemy! \n",
    "response.extra_info[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
